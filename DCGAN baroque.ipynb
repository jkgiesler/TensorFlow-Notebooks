{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCGAN implementation in Keras\n",
    "\n",
    "Inspired by jacobgil's implementation: [https://github.com/jacobgil/keras-dcgan/](https://github.com/jacobgil/keras-dcgan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3226, 120, 120, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import cPickle as pickle\n",
    "\n",
    "# with open('shrunk_baroque.pkl','rb') as f:\n",
    "#     imgs = pickle.load(f)\n",
    "# imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "pylab import has clobbered these variables: ['f']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function,division\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Reshape\n",
    "from keras.layers.core import Activation\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.convolutional import UpSampling2D\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import mnist\n",
    "from tqdm import tqdm_notebook \n",
    "import numpy as np\n",
    "\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO(jkg): add better descriptions of models\n",
    "\n",
    "def keras_generator_model():\n",
    "    '''generate images\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    ##########\n",
    "    model.add(Dense(input_dim = 100, output_dim = 1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    ##########\n",
    "    model.add(Dense(128*7*7))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Activation('tanh'))\n",
    "    ##########\n",
    "    model.add(Reshape((128,7,7),input_shape=(128*7*7,)))\n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    model.add(Convolution2D(64,5,5,border_mode = 'same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    ##########\n",
    "    model.add(UpSampling2D(size=(2,2)))\n",
    "    model.add(Convolution2D(1,5,5,border_mode='same'))\n",
    "    model.add(Activation('tanh'))\n",
    "    return model\n",
    "\n",
    "def keras_discriminator_model():\n",
    "    '''discriminate images\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(64, 5, 5,\n",
    "                           border_mode = 'same',\n",
    "                           input_shape = (1,28,28)))\n",
    "    model.add(Activation('tanh'))\n",
    "    ##########\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Convolution2D(128, 5, 5))\n",
    "    model.add(Activation('tanh'))\n",
    "    ##########\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Flatten())\n",
    "    ##########\n",
    "    model.add(Dense(1024))\n",
    "    model.add(Activation('tanh'))\n",
    "    ##########\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    return model\n",
    "\n",
    "def generator_and_discriminator(gen_model,dis_model):\n",
    "    '''stack the generator model on top of the discriminator model\n",
    "    only allow the generator model to be trainable\n",
    "    '''\n",
    "    model = Sequential()\n",
    "    model.add(gen_model)\n",
    "    dis_model.trainable = False\n",
    "    model.add(dis_model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing model w/ MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 1, 28, 28)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize and Reshape data\n",
    "(X_train,y_train),(X_test,y_test) = mnist.load_data()\n",
    "\n",
    "# why do this instead of convert to [0,1] - center and scale not just scale\n",
    "X_train_n = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "# X_train = X_train / 255 <-\n",
    "\n",
    "X_train_n = X_train_n.reshape((X_train.shape[0],1,X_train.shape[1],X_train.shape[2]))\n",
    "# new shape should be (60000,1,28,28) 1 channel image 28x28 px\n",
    "X_train_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "checkpoint_folder = 'chk/'\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "batchs_per_epoch = int(X_train_n.shape[0]/batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Instantiate Models\n",
    "\n",
    "desc = keras_discriminator_model()\n",
    "gen = keras_generator_model()\n",
    "d_on_g = generator_and_discriminator(gen,desc)\n",
    "\n",
    "d_op = SGD(lr = 5e-4,momentum = 0.9,nesterov=True)\n",
    "g_op = SGD(lr = 5e-4,momentum = 0.9,nesterov=True)\n",
    "\n",
    "gen.compile(loss='binary_crossentropy',optimizer = \"SGD\")\n",
    "\n",
    "d_on_g.compile(loss='binary_crossentropy',optimizer = g_op)\n",
    "\n",
    "desc.trainable = True # necessary??\n",
    "desc.compile(loss='binary_crossentropy',optimizer= d_op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "noise = np.zeros((batch_size,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avg desc loss 0.0808791592717 || avg gen loss 3.05545806885\n",
      "avg desc loss 0.0076539455913 || avg gen loss 4.38770008087\n",
      "avg desc loss 0.00242618238553 || avg gen loss 5.41740655899\n",
      "avg desc loss 0.00132587447297 || avg gen loss 5.99391412735\n",
      "avg desc loss 0.000883951608557 || avg gen loss 6.38273286819\n",
      "avg desc loss 0.00065121910302 || avg gen loss 6.6802945137\n",
      "avg desc loss 0.00051131186774 || avg gen loss 6.91629648209\n",
      "avg desc loss 0.000417206989368 || avg gen loss 7.11600780487\n",
      "avg desc loss 0.000351458176738 || avg gen loss 7.28294897079\n",
      "avg desc loss 0.000302845583064 || avg gen loss 7.42806386948\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-a335813fd4cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# train desc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0md_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdesc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_desc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[1;31m#build noise up\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/keras/models.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, class_weight, sample_weight, **kwargs)\u001b[0m\n\u001b[0;32m    519\u001b[0m         return self.model.train_on_batch(x, y,\n\u001b[0;32m    520\u001b[0m                                          \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 521\u001b[1;33m                                          class_weight=class_weight)\n\u001b[0m\u001b[0;32m    522\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    523\u001b[0m     def test_on_batch(self, x, y,\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1221\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1223\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1224\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1225\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    743\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[0msession\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 745\u001b[1;33m         \u001b[0mupdated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    746\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    747\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    708\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    709\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 710\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    711\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    712\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    906\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    907\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 908\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    909\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    910\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    956\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    957\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 958\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    959\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    960\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    963\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    964\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 965\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    966\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    967\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/jgiesler/anaconda2/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    945\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    946\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 947\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    948\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epoch_loss = []\n",
    "for epoch in tqdm_notebook(range(epochs),desc = 'epoch'):\n",
    "    batch_loss = []\n",
    "    for batch_index in tqdm_notebook(range(batchs_per_epoch),desc = 'batch',leave=False):\n",
    "        \n",
    "        #build noise up\n",
    "        for sample in range(batch_size):\n",
    "            noise[sample,:] = np.random.uniform(-1,1,100)\n",
    "        \n",
    "        image_set = X_train_n[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "        \n",
    "        # feed noise into generator network\n",
    "        gen_images = gen.predict(noise,verbose=0)\n",
    "\n",
    "        # now we have data for our desc network\n",
    "        X_desc = np.concatenate([image_set,gen_images],axis=0)\n",
    "        y = [1] * batch_size + [0] * batch_size\n",
    "        \n",
    "        # train desc\n",
    "        d_loss = desc.train_on_batch(X_desc,y)\n",
    "        \n",
    "        #build noise up\n",
    "        for sample in range(batch_size):\n",
    "            noise[sample,:] = np.random.uniform(-1,1,100)\n",
    "            \n",
    "        # train generator on improved desc\n",
    "        desc.trainable = False\n",
    "        g_loss = d_on_g.train_on_batch(noise,[1] * batch_size)\n",
    "        desc.trainable = True\n",
    "        \n",
    "        if(batch_index+1 == batchs_per_epoch):\n",
    "            gen.save_weights(checkpoint_folder+'g_weights.h5',overwrite=True)\n",
    "            desc.save_weights(checkpoint_folder+'d_weights.h5',overwrite=True)\n",
    "        batch_loss.append([d_loss,g_loss])\n",
    "    \n",
    "    avg_d = np.mean([x[0] for x in batch_loss])\n",
    "    avg_g = np.mean([x[1] for x in batch_loss])\n",
    "    print('avg desc loss {} || avg gen loss {}'.format(avg_d,avg_g))\n",
    "    epoch_loss.append([avg_d,avg_g])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
